{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b71b36",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e79cc13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader, PyMuPDFLoader, CSVLoader, BSHTMLLoader,UnstructuredXMLLoader, PythonLoader\n",
    "\n",
    "def process_all_docs(data_directory):\n",
    "    loaders = {\n",
    "        \".txt\": TextLoader,\n",
    "        \".pdf\": PyMuPDFLoader,\n",
    "        \".csv\": CSVLoader,\n",
    "        \".html\": BSHTMLLoader,\n",
    "        \".xml\": UnstructuredXMLLoader,\n",
    "        \".py\": PythonLoader,           \n",
    "    }\n",
    "    summary = []\n",
    "    all_documents = []\n",
    "\n",
    "    # Walk directory recursively to handle nested folders\n",
    "    for root, _, files in os.walk(data_directory):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[1].lower()\n",
    "            loader_cls = loaders.get(ext)\n",
    "            if not loader_cls:\n",
    "                summary.append((filename, ext, \"SKIPPED (not supported)\", 0))\n",
    "                continue\n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                loader = loader_cls(file_path)\n",
    "                docs = loader.load()\n",
    "                all_documents.extend(docs)\n",
    "                summary.append((filename, ext, \"OK\", len(docs)))\n",
    "                print(f\"✓ {filename}: {len(docs)} docs\")\n",
    "            except Exception as e:\n",
    "                summary.append((filename, ext, f\"ERROR ({e})\", 0))\n",
    "                print(f\"✗ {filename}: {e}\")\n",
    "\n",
    "    # Print summary table\n",
    "    print(\"\\n--- Ingestion Summary ---\")\n",
    "    print(f\"{'File':50} {'Ext':5} {'Status':25} {'Docs'}\")\n",
    "    for s in summary:\n",
    "        print(f\"{s[0]:50} {s[1]:5} {s[2]:25} {s[3]}\")\n",
    "    print(f\"\\nTotal loaded documents: {len(all_documents)}\")\n",
    "    return all_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af744dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11cafaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pride and Prejudice Author Jane Austen.pdf: 516 docs\n",
      "\n",
      "--- Ingestion Summary ---\n",
      "File                                               Ext   Status                    Docs\n",
      "Pride and Prejudice Author Jane Austen.pdf         .pdf  OK                        516\n",
      "\n",
      "Total loaded documents: 516\n",
      "Split 516 documents into 989 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: PRIDE AND \n",
      "PREJUDICE\n",
      "Jane Austen\n",
      "InfoBooks.org...\n",
      "Metadata: {'producer': '3-Heights™ PDF Optimization Shell 6.3.1.5 (http://www.pdf-tools.com)', 'creator': 'Adobe Acrobat Pro DC 20.9.20063', 'creationdate': '2022-07-01T09:23:19-04:00', 'source': '../data/all_files\\\\Pride and Prejudice Author Jane Austen.pdf', 'file_path': '../data/all_files\\\\Pride and Prejudice Author Jane Austen.pdf', 'total_pages': 516, 'format': 'PDF 1.7', 'title': 'pride and prejudice', 'author': 'jane austen', 'subject': '', 'keywords': 'pride and prejudice by jane austen, pride, prejudice, jane austen, pride and prejudice, jane', 'moddate': '2024-03-13T17:08:16-03:00', 'trapped': '', 'modDate': \"D:20240313170816-03'00'\", 'creationDate': \"D:20220701092319-04'00'\", 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "all_documents = process_all_docs(\"../data/all_files\")\n",
    "chunks=split_documents(all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdfa631",
   "metadata": {},
   "source": [
    "## Embedding Data into VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9993264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding and VectorDB\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb \n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,Dict,Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466257b",
   "metadata": {},
   "source": [
    "### EmbeddingManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de2a2076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embedding Model:all-MiniLM-L6-v2\n",
      "Model Loaded Succesfully , Embedding Dimensions :384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x298805c9160>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles Document Embedding Generation using SentenceTransformer\"\"\"\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading Embedding Model:{self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model Loaded Succesfully , Embedding Dimensions :{self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error:{e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: list) -> np.ndarray: \n",
    "        \"\"\"Generate an embedding vector for the given text\"\"\"\n",
    "        if self.model is None: \n",
    "            raise ValueError(\"ModelNotLoaded\")\n",
    "        print(f\"Generating Embeddings for {len(texts)} text(s)...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated Embedding Model with shape {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "# Start the embedding manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a7c76",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4492e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChromaDB Client...\n",
      "Collection 'pdf_documents' is ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x298805c9550>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages documents embedding in a chromaDB vector store \"\"\"\n",
    "\n",
    "    def __init__(self,collection_name: str = \"pdf_documents\",persist_directory: str = \"../data/vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "    \n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize the ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create Persistent Directory\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            # Get or create collection\n",
    "            print(\"Initializing ChromaDB Client...\")\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\":\"PDF Document Collection\"}\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' is ready.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing ChromaDB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Document], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents to the vector store after generating embeddings\"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match the number of embeddings\")\n",
    "        \n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df69b16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Embeddings for 989 text(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 31/31 [00:21<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embedding Model with shape (989, 384)\n",
      "Successfully added 989 documents to vector store\n",
      "Total documents in collection: 11868\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d459a0",
   "metadata": {},
   "source": [
    "## Retriver pipeline from Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d992deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query based retrival from Vector Store\"\"\"\n",
    "    def __init__(self, vector_store : VectorStore, embedding_manager : EmbeddingManager):\n",
    "        \"\"\"Initialize the retriver\n",
    "        \n",
    "        Args:\n",
    "            vector_store: vector store for containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f158138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x298ca6ea7b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "614fd56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_retriever.retrieve(\"what is intra-task forgetting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7adb6",
   "metadata": {},
   "source": [
    "## LLM with RAG integrtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60586525",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple RAG LLM \n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(groq_api_key=groq_api_key,model_name=\"gemma2-9b-it\",temperature=0.1,max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c7b5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=\"\"\"Carefully read the provided context in {context} and answer the question: {query}\n",
    "\n",
    "    If the answer is found, quote the relevant text directly and reference its location (chapter, page, or section if given).\n",
    "\n",
    "    If helpful, include 1–2 surrounding sentences.\n",
    "\n",
    "    If the answer must be combined from different parts, use only context quotes.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdd354d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Who is Mrs. Long?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating Embeddings for 1 text(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embedding Model with shape (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mrs. Long is a friend of Mrs. Bennet who is described as \"as good a creature as ever lived.\" \n",
      "\n",
      "Here's the relevant text:\n",
      "\n",
      "\"I do think Mrs. Long is as good a creature as ever lived—and her nieces are very pretty behaved girls, and not at all handsome: I like them prodigiously.” \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"Who is Mrs. Long?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0ae76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63fa4de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGapp (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
